"""
å†œä¸šæ”¿ç­–æ™ºèƒ½é—®ç­” Agent
å¢å¼ºåŠŸèƒ½ï¼š
- å¤šè½®åŠ¨æ€è¿½é—®å¼•å¯¼ï¼ˆæ ¹æ®æ„å›¾åªè¿½é—®å¿…è¦ä¿¡æ¯ï¼‰
- Streamlit Webå¯è§†åŒ–ç•Œé¢ï¼ˆæ›¿ä»£CLIï¼Œæ›´å‹å¥½ï¼‰
- åŸæœ‰æ ¸å¿ƒåŠŸèƒ½å…¨éƒ¨ä¿ç•™
"""

import os
import re
from typing import List, Optional, Literal, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime
import streamlit as st

# LangChain ç›¸å…³å¯¼å…¥
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    AIMessage,
    SystemMessage
)
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from langchain_community.vectorstores.faiss import FAISS
import dotenv

# =========================
# ç¯å¢ƒå˜é‡åŠ è½½ & é…ç½®é¡¹
# =========================
dotenv.load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")
if not OPENAI_API_KEY:
    raise EnvironmentError("æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼")

# å¸¸é‡å®šä¹‰
LLM_MODEL = "gpt-4o-mini"
LLM_TEMPERATURE = 0.2
RAG_TOP_K = 3
FAISS_INDEX_PATH = "faiss_index"

# è®°å¿†é…ç½®
SHORT_MEMORY_TOP_K = 5
SUMMARY_TRIGGER_ROUNDS = 3

# é€šç”¨æ„å›¾å…³é”®è¯
GREETING_KEYWORDS = ["ä½ å¥½", "æ‚¨å¥½", "å—¨", "å“ˆå–½", "æ—©ä¸Šå¥½", "ä¸‹åˆå¥½", "æ™šä¸Šå¥½"]
THANKS_KEYWORDS = ["è°¢è°¢", "æ„Ÿè°¢", "å¤šè°¢", "è¾›è‹¦äº†"]
FAREWELL_KEYWORDS = ["å†è§", "æ‹œæ‹œ", "ä¸‹æ¬¡è§", "å›è§"]
IDENTITY_KEYWORDS = ["ä½ æ˜¯è°", "ä½ å«ä»€ä¹ˆ", "åå­—", "èº«ä»½"]
FUNCTION_KEYWORDS = ["ä½ èƒ½åšä»€ä¹ˆ", "åŠŸèƒ½", "èƒ½å¹²ä»€ä¹ˆ", "å¸®åŠ©", "ä½œç”¨"]
GENERAL_KEYWORDS = GREETING_KEYWORDS + THANKS_KEYWORDS + FAREWELL_KEYWORDS + IDENTITY_KEYWORDS + FUNCTION_KEYWORDS

# é•¿è®°å¿†æ‘˜è¦æç¤ºè¯
SUMMARY_PROMPT = """
è¯·æ€»ç»“ä»¥ä¸‹å†œä¸šæ”¿ç­–é—®ç­”å¯¹è¯çš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¦æ±‚ï¼š
1. ä¿ç•™å…³é”®ä¿¡æ¯ï¼šç”¨æˆ·å…³æ³¨çš„åœ°åŒºã€ä½œç‰©ã€æ”¿ç­–ç±»å‹ã€æ ¸å¿ƒé—®é¢˜
2. å»é™¤å†—ä½™å†…å®¹ï¼Œåªä¿ç•™æœ‰ä»·å€¼çš„ä¿¡æ¯
3. æ ¼å¼ç®€æ´ï¼Œä½¿ç”¨è¦ç‚¹å¼æ€»ç»“
4. å¿½ç•¥æ— å…³çš„å¯’æš„å†…å®¹

å¯¹è¯å†å²ï¼š
{conversation_history}

å½“å‰æ—¶é—´ï¼š{current_time}

æ€»ç»“è¦æ±‚ï¼šä»…è¾“å‡ºæ€»ç»“å†…å®¹ï¼Œä¸è¦é¢å¤–è§£é‡Š
"""

# =========================
# å·¥å…·å‡½æ•°
# =========================
def trim_short_memory(messages: List[BaseMessage], top_k: int = SHORT_MEMORY_TOP_K) -> List[BaseMessage]:
    """æ‰‹åŠ¨ä¿®å‰ªçŸ­è®°å¿†ï¼Œå…¼å®¹æ‰€æœ‰ LangChain ç‰ˆæœ¬"""
    if not messages:
        return []
    system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]
    conversation_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
    keep_count = top_k * 2
    trimmed_conversation = conversation_messages if len(conversation_messages) <= keep_count else conversation_messages[-keep_count:]
    return system_messages + trimmed_conversation

def generate_long_memory_summary(messages: List[BaseMessage], llm: ChatOpenAI) -> str:
    """ç”Ÿæˆé•¿è®°å¿†æ‘˜è¦"""
    conv_history = ""
    for msg in messages:
        if isinstance(msg, HumanMessage):
            conv_history += f"ç”¨æˆ·ï¼š{msg.content}\n"
        elif isinstance(msg, AIMessage):
            conv_history += f"AIï¼š{msg.content}\n"
    prompt = PromptTemplate(template=SUMMARY_PROMPT, input_variables=["conversation_history", "current_time"])
    summary_input = prompt.format(
        conversation_history=conv_history,
        current_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
    response = llm.invoke([HumanMessage(content=summary_input)])
    return response.content.strip()

# =========================
# æ•°æ®æ¨¡å‹å®šä¹‰
# =========================
class AgentState(BaseModel):
    messages: List[BaseMessage] = Field(default_factory=list)
    user_question: Optional[str] = None
    intent_type: Optional[Literal[
        "greeting",          # é—®å€™
        "thanks",            # æ„Ÿè°¢
        "farewell",          # å‘Šåˆ«
        "identity",          # èº«ä»½è¯¢é—®
        "function",          # åŠŸèƒ½è¯¢é—®
        "policy_explanation",# æ”¿ç­–è§£è¯»
        "eligibility_check", # èµ„æ ¼æ ¸æŸ¥
        "calculation",       # é‡‘é¢è®¡ç®—
        "procedure",         # åŠç†æµç¨‹
        "unclear"            # æ„å›¾ä¸æ˜
    ]] = None
    short_term_facts: Dict[str, Any] = Field(default_factory=dict)
    long_term_profile: Dict[str, Any] = Field(default_factory=lambda: {"summary": "", "conversation_round": 0})
    need_rag: bool = False
    retrieved_docs: List[Dict[str, Any]] = Field(default_factory=list)
    need_clarification: bool = False
    refuse_answer: bool = False
    final_answer: Optional[str] = None

# =========================
# LangGraph èŠ‚ç‚¹å‡½æ•°
# =========================
def parse_user_input(state: AgentState) -> AgentState:
    """è§£æç”¨æˆ·è¾“å…¥ï¼Œæå–é—®é¢˜"""
    for msg in reversed(state.messages):
        if isinstance(msg, HumanMessage):
            state.user_question = msg.content.strip()
            break
    state.long_term_profile["conversation_round"] = state.long_term_profile.get("conversation_round", 0) + 1
    state.messages = trim_short_memory(state.messages, SHORT_MEMORY_TOP_K)
    return state

def classify_intent(state: AgentState) -> AgentState:
    """æ„å›¾åˆ†ç±»èŠ‚ç‚¹ï¼šä¼˜å…ˆè¯†åˆ«æ‰€æœ‰é€šç”¨è¯æœ¯ï¼Œå†è¯†åˆ«æ”¿ç­–æ„å›¾"""
    user_question = state.user_question or ""

    # é€šç”¨æ„å›¾åˆ¤æ–­
    if any(word in user_question for word in GREETING_KEYWORDS):
        state.intent_type = "greeting"
        state.need_rag = False
        state.need_clarification = False
        return state
    elif any(word in user_question for word in THANKS_KEYWORDS):
        state.intent_type = "thanks"
        state.need_rag = False
        state.need_clarification = False
        return state
    elif any(word in user_question for word in FAREWELL_KEYWORDS):
        state.intent_type = "farewell"
        state.need_rag = False
        state.need_clarification = False
        return state
    elif any(word in user_question for word in IDENTITY_KEYWORDS):
        state.intent_type = "identity"
        state.need_rag = False
        state.need_clarification = False
        return state
    elif any(word in user_question for word in FUNCTION_KEYWORDS):
        state.intent_type = "function"
        state.need_rag = False
        state.need_clarification = False
        return state

    # æ”¿ç­–ç›¸å…³æ„å›¾åˆ¤æ–­
    if any(keyword in user_question for keyword in ["è¡¥è´´", "æ”¿ç­–", "è§„å®š", "æ–‡ä»¶"]):
        state.intent_type = "policy_explanation"
        state.need_rag = True
    elif any(keyword in user_question for keyword in ["èƒ½ä¸èƒ½", "ç¬¦åˆ", "èµ„æ ¼", "æ¡ä»¶"]):
        state.intent_type = "eligibility_check"
        state.need_rag = True
    elif any(keyword in user_question for keyword in ["å¤šå°‘é’±", "æ€ä¹ˆç®—", "é‡‘é¢", "æ ‡å‡†"]):
        state.intent_type = "calculation"
        state.need_rag = True
    elif any(keyword in user_question for keyword in ["å»å“ª", "ä»€ä¹ˆæ—¶å€™", "æ€ä¹ˆç”³è¯·", "æµç¨‹"]):
        state.intent_type = "procedure"
        state.need_rag = True
    else:
        state.intent_type = "unclear"
        state.need_clarification = True

    return state

def general_response_node(state: AgentState) -> AgentState:
    """é€šç”¨å›å¤èŠ‚ç‚¹ï¼Œå¤„ç†é—®å€™/æ„Ÿè°¢/å‘Šåˆ«/èº«ä»½/åŠŸèƒ½è¯¢é—®"""
    intent = state.intent_type
    responses = {
        "greeting": "æ‚¨å¥½ï¼æˆ‘æ˜¯å†œä¸šæ”¿ç­–æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨è§£ç­”çš„å†œä¸šæ”¿ç­–é—®é¢˜å—ï¼ŸğŸ˜Š",
        "thanks": "ä¸å®¢æ°”ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–å†œä¸šæ”¿ç­–ç›¸å…³çš„é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘å“¦~",
        "farewell": "å†è§ï¼æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼Œç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ï¼ğŸ‘‹",
        "identity": "æˆ‘æ˜¯å†œä¸šæ”¿ç­–æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œä¸“ä¸ºæ‚¨è§£ç­”å„ç±»å†œä¸šç›¸å…³çš„æ”¿ç­–é—®é¢˜ï¼Œæ¯”å¦‚è¡¥è´´æ ‡å‡†ã€ç”³è¯·æµç¨‹ã€èµ„æ ¼æ¡ä»¶ç­‰~",
        "function": "æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å†œä¸šæ”¿ç­–ç›¸å…³çš„å„ç±»é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼š\n1. å„ç±»å†œä¸šè¡¥è´´çš„æ ‡å‡†å’Œç”³è¯·æ¡ä»¶\n2. å†œä¸šé¡¹ç›®çš„ç”³æŠ¥æµç¨‹\n3. ç›¸å…³æ”¿ç­–æ–‡ä»¶çš„è§£è¯»\næ‚¨å¯ä»¥ç›´æ¥å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£çš„å†…å®¹~"
    }
    state.final_answer = responses.get(intent, "å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼")
    state.messages.append(AIMessage(content=state.final_answer))
    return state

def clarification_node(state: AgentState) -> AgentState:
    """
    æ ¸å¿ƒä¼˜åŒ–ï¼šå¤šè½®åŠ¨æ€è¿½é—®å¼•å¯¼
    æ ¹æ®ä¸åŒæ„å›¾ï¼Œåªè¿½é—®å¿…è¦çš„ä¿¡æ¯ï¼Œè€Œéä¸€æ¬¡æ€§åˆ—å‡ºæ‰€æœ‰è¦æ±‚
    """
    user_question = state.user_question or ""
    intent = state.intent_type
    long_memory = state.long_term_profile.get("summary", "")

    # ä»å†å²å¯¹è¯/å½“å‰é—®é¢˜ä¸­æå–å·²æœ‰çš„ä¿¡æ¯
    has_region = bool(re.search(r"[çœå¸‚å¿]", user_question)) or ("åœ°åŒº" in long_memory)
    has_crop = bool(re.search(r"å°éº¦|ç‰ç±³|æ°´ç¨»|è”¬èœ|å¤§è±†", user_question)) or ("ä½œç‰©" in long_memory)
    has_year = bool(re.search(r"20\d{2}", user_question)) or ("å¹´ä»½" in long_memory)

    # æŒ‰æ„å›¾åŠ¨æ€ç”Ÿæˆè¿½é—®è¯æœ¯
    clarify_map = {
        # æ”¿ç­–è§£è¯»ï¼šä¼˜å…ˆè¿½é—®åœ°åŒºï¼ˆæ ¸å¿ƒï¼‰
        "policy_explanation":
            "è¯·é—®æ‚¨æƒ³æŸ¥è¯¢å“ªä¸ªçœ/å¸‚/å¿çš„æ”¿ç­–å‘¢ï¼Ÿ" if not has_region else
            ("è¯·é—®æ‚¨å…³æ³¨çš„æ˜¯å“ªä¸€å¹´çš„æ”¿ç­–ï¼ˆå¦‚2025ï¼‰ï¼Ÿ" if not has_year else
             "è¯·é—®æ‚¨æƒ³äº†è§£å“ªç§ä½œç‰©çš„æ”¿ç­–ï¼ˆå¦‚å°éº¦ã€ç‰ç±³ï¼‰ï¼Ÿ" if not has_crop else
             "è¯·è¡¥å……æ‚¨æƒ³äº†è§£çš„å…·ä½“æ–¹å‘ï¼ˆå¦‚è¡¥è´´æ ‡å‡†ã€ç”³è¯·æ¡ä»¶ï¼‰~"),

        # èµ„æ ¼æ ¸æŸ¥ï¼šä¼˜å…ˆè¿½é—®åœ°åŒº+ä½œç‰©
        "eligibility_check":
            "è¯·é—®æ‚¨çš„ç§æ¤åœ°åŒºæ˜¯å“ªä¸ªçœ/å¸‚/å¿ï¼Œç§æ¤çš„æ˜¯ä»€ä¹ˆä½œç‰©å‘¢ï¼Ÿ" if not has_region or not has_crop else
            ("è¯·é—®æ‚¨æƒ³æŸ¥è¯¢å“ªä¸€å¹´çš„èµ„æ ¼æ¡ä»¶ï¼Ÿ" if not has_year else
             "è¯·è¡¥å……æ›´å¤šä¿¡æ¯ï¼ˆå¦‚ç§æ¤é¢ç§¯ã€æ˜¯å¦ç¬¦åˆåŸºæœ¬æ¡ä»¶ï¼‰~"),

        # é‡‘é¢è®¡ç®—ï¼šä¼˜å…ˆè¿½é—®åœ°åŒº+ä½œç‰©+é¢ç§¯
        "calculation":
            "è¯·é—®æ‚¨çš„ç§æ¤åœ°åŒºã€ä½œç‰©ç±»å‹å’Œç§æ¤é¢ç§¯åˆ†åˆ«æ˜¯å¤šå°‘å‘¢ï¼Ÿ" if not has_region or not has_crop else
            ("è¯·é—®æ‚¨æƒ³æŒ‰å“ªä¸€å¹´çš„è¡¥è´´æ ‡å‡†è®¡ç®—ï¼Ÿ" if not has_year else
             "è¯·è¡¥å……ç§æ¤é¢ç§¯ï¼Œæˆ‘æ¥å¸®æ‚¨è®¡ç®—è¡¥è´´æ€»é¢~"),

        # æµç¨‹æŸ¥è¯¢ï¼šä¼˜å…ˆè¿½é—®åœ°åŒº
        "procedure":
            "è¯·é—®æ‚¨æ‰€åœ¨çš„åœ°åŒºæ˜¯å“ªä¸ªçœ/å¸‚/å¿ï¼Ÿ" if not has_region else
            ("è¯·é—®æ‚¨æƒ³äº†è§£å“ªä¸€å¹´çš„ç”³è¯·æµç¨‹ï¼Ÿ" if not has_year else
             "è¯·é—®æ‚¨æƒ³äº†è§£å“ªç§ä½œç‰©çš„ç”³è¯·æµç¨‹ï¼Ÿ" if not has_crop else
             "è¯·è¡¥å……æ‚¨æƒ³äº†è§£çš„å…·ä½“æµç¨‹ç¯èŠ‚ï¼ˆå¦‚ç”³æŠ¥æ—¶é—´ã€æ‰€éœ€ææ–™ï¼‰~"),

        # å®Œå…¨ä¸æ˜çš„é—®é¢˜ï¼šç®€åŒ–è¿½é—®
        "unclear": "ä¸ºäº†ç²¾å‡†å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·è¡¥å……ï¼š\n1. æ‰€åœ¨åœ°åŒºï¼ˆçœ/å¸‚/å¿ï¼‰\n2. æ¶‰åŠçš„ä½œç‰©ç±»å‹ï¼ˆå¦‚å°éº¦ã€ç‰ç±³ï¼‰"
    }

    # ç”Ÿæˆæœ€ç»ˆè¿½é—®è¯æœ¯
    clarify_msg = clarify_map.get(intent, clarify_map["unclear"])
    # è¡¥å……è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
    if long_memory and intent != "unclear":
        clarify_msg = f"æ ¹æ®ä¹‹å‰çš„å¯¹è¯ï¼Œ{clarify_msg}"

    state.final_answer = clarify_msg
    state.messages.append(AIMessage(content=clarify_msg))
    return state

def rag_retrieval_node(state: AgentState, vectorstore: FAISS) -> AgentState:
    """RAG æ£€ç´¢èŠ‚ç‚¹"""
    if state.need_rag and state.user_question:
        try:
            retrieved_documents = vectorstore.similarity_search(state.user_question, k=RAG_TOP_K)
            state.retrieved_docs = [
                {"page_content": doc.page_content.strip(), "source": doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")}
                for doc in retrieved_documents
            ]
        except Exception as e:
            print(f"RAG æ£€ç´¢å‡ºé”™: {e}")
            state.retrieved_docs = []
    return state

def llm_expert_answer(state: AgentState) -> AgentState:
    """æ”¿ç­–å›ç­”èŠ‚ç‚¹"""
    long_memory = state.long_term_profile.get("summary", "")
    memory_context = f"\nã€å¯¹è¯å†å²æ€»ç»“ã€‘ï¼š{long_memory}\n" if long_memory else ""

    system_prompt = f"""
ä½ æ˜¯å†œä¸šå†œæ‘æ”¿ç­–ä¸“å®¶ï¼Œè¯·ã€ä¸¥æ ¼éµå®ˆã€‘ä»¥ä¸‹è§„åˆ™ï¼š
1. åªèƒ½åŸºäºã€æ”¿ç­–åŸæ–‡è¯æ®ã€‘å’Œã€å¯¹è¯å†å²ã€‘å›ç­”
2. æ¯ä¸€ä¸ªç»“è®ºï¼Œå¿…é¡»å…ˆå¼•ç”¨åŸæ–‡å¥å­ï¼Œæ ¼å¼ä¸ºï¼š
   ã€æ”¿ç­–åŸæ–‡ã€‘â€¦â€¦
   ã€é€šä¿—è§£è¯»ã€‘â€¦â€¦
3. æ— æ˜ç¡®ä¾æ®æ—¶ï¼Œå›ç­”ï¼šæœªåœ¨å·²æ£€ç´¢æ”¿ç­–ä¸­æ‰¾åˆ°æ˜ç¡®ä¾æ®ï¼Œå»ºè®®å’¨è¯¢å½“åœ°å†œä¸šå†œæ‘å±€
4. ç¦æ­¢è‡ªè¡Œæ¨æ–­ã€è¡¥å…¨å¸¸è¯†ã€ç¼–é€ å†…å®¹

ã€å¯¹è¯ä¸Šä¸‹æ–‡ã€‘
{memory_context}

å›ç­”è¯­è¨€è¦æœ´å®ï¼Œé¢å‘å†œæˆ·ã€‚
"""

    # æ„é€ è¯æ®
    evidence_blocks = ""
    if state.retrieved_docs:
        aggregated = aggregate_sentences(state.retrieved_docs)
        evidence_blocks = "\nã€å¯ç”¨æ”¿ç­–è¯æ®ã€‘\n"
        for i, item in enumerate(aggregated, 1):
            evidence_blocks += f"\nã€è¯æ®{i}ï½œæ¥æºï¼š{item['source']}ã€‘\n{item['content']}\n"

    # è°ƒç”¨LLM
    llm = ChatOpenAI(
        model=LLM_MODEL,
        temperature=0.1,
        api_key=OPENAI_API_KEY,
        base_url=OPENAI_BASE_URL
    )
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=evidence_blocks),
        HumanMessage(content=f"ç”¨æˆ·é—®é¢˜ï¼š{state.user_question}")
    ]
    response = llm.invoke(messages)

    state.final_answer = response.content
    state.messages.append(AIMessage(content=response.content))
    return state

def update_long_memory(state: AgentState) -> AgentState:
    """æ›´æ–°é•¿è®°å¿†èŠ‚ç‚¹"""
    current_round = state.long_term_profile.get("conversation_round", 0)

    if current_round % SUMMARY_TRIGGER_ROUNDS == 0 and current_round > 0:
        # Streamlitä¸­ç”¨st.infoæ›¿ä»£print
        st.info(f"ğŸ” æ­£åœ¨æ›´æ–°å¯¹è¯è®°å¿†ï¼ˆç¬¬ {current_round} è½®ï¼‰...")

        llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=0.1,
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        new_summary = generate_long_memory_summary(state.messages, llm)

        # åˆå¹¶æ–°æ—§æ‘˜è¦
        old_summary = state.long_term_profile.get("summary", "")
        if old_summary:
            state.long_term_profile["summary"] = f"å†å²æ€»ç»“ï¼š{old_summary}\næœ€æ–°æ€»ç»“ï¼š{new_summary}"
        else:
            state.long_term_profile["summary"] = new_summary

        st.success(f"ğŸ“ è®°å¿†æ›´æ–°å®Œæˆï¼š{state.long_term_profile['summary'][:100]}...")

    return state

def aggregate_sentences(docs: List[Dict[str, Any]], window: int = 1) -> List[Dict[str, Any]]:
    """èšåˆå‘½ä¸­å¥å­ä¸ºå¼±æ®µè½"""
    aggregated = []
    for i, doc in enumerate(docs):
        sentences = [doc["page_content"]]
        if i - window >= 0:
            sentences.insert(0, docs[i - window]["page_content"])
        if i + window < len(docs):
            sentences.append(docs[i + window]["page_content"])
        aggregated.append({
            "content": "\n".join(sentences),
            "evidence": doc["page_content"],
            "source": doc.get("source", "æœªçŸ¥æ–‡ä»¶")
        })
    return aggregated

# =========================
# æ„å»º LangGraph å·¥ä½œæµ
# =========================
def build_agricultural_policy_agent(vectorstore: FAISS):
    """æ„å»ºå¸¦è®°å¿†å’Œå¢å¼ºé€šç”¨èƒ½åŠ›çš„Agent"""
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("parse_input", parse_user_input)
    workflow.add_node("classify_intent", classify_intent)
    workflow.add_node("general_response", general_response_node)
    workflow.add_node("update_long_memory", update_long_memory)
    workflow.add_node("clarify", clarification_node)
    workflow.add_node("rag_retrieval", lambda s: rag_retrieval_node(s, vectorstore))
    workflow.add_node("generate_answer", llm_expert_answer)

    # è®¾ç½®å…¥å£èŠ‚ç‚¹
    workflow.set_entry_point("parse_input")

    # å®šä¹‰æ‰§è¡Œæµç¨‹
    workflow.add_edge("parse_input", "classify_intent")

    # æ„å›¾è·¯ç”±å‡½æ•°
    def route_intent(state: AgentState) -> str:
        if state.intent_type in ["greeting", "thanks", "farewell", "identity", "function"]:
            return "general_response"
        elif state.need_clarification:
            return "clarify"
        else:
            return "rag_retrieval"

    # æ¡ä»¶åˆ†æ”¯
    workflow.add_conditional_edges(
        source="classify_intent",
        path=route_intent,
        path_map={
            "general_response": "general_response",
            "clarify": "clarify",
            "rag_retrieval": "rag_retrieval"
        }
    )

    # åç»­æµç¨‹
    workflow.add_edge("general_response", "update_long_memory")
    workflow.add_edge("rag_retrieval", "generate_answer")
    workflow.add_edge("generate_answer", "update_long_memory")
    workflow.add_edge("clarify", "update_long_memory")
    workflow.add_edge("update_long_memory", END)

    return workflow.compile()

# =========================
# Streamlit Webç•Œé¢ï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰
# =========================
def streamlit_chat_interface():
    """
    æ›¿ä»£CLIçš„Webå¯è§†åŒ–ç•Œé¢
    - å‹å¥½çš„å¯¹è¯ç•Œé¢
    - ä¿å­˜å¯¹è¯å†å²
    - é€‚é…å¤šè½®è¿½é—®
    """
    # é¡µé¢åŸºç¡€é…ç½®
    st.set_page_config(
        page_title="å†œä¸šæ”¿ç­–æ™ºèƒ½é—®ç­”åŠ©æ‰‹",
        page_icon="ğŸŒ¾",
        layout="wide"
    )

    st.title("ğŸŒ¾ å†œä¸šæ”¿ç­–æ™ºèƒ½é—®ç­”åŠ©æ‰‹")
    st.markdown("### ä¸“æ³¨è§£ç­”å„ç±»å†œä¸šæ”¿ç­–é—®é¢˜ï¼ˆè¡¥è´´ã€ç”³è¯·ã€èµ„æ ¼ç­‰ï¼‰")
    st.divider()

    # 1. åˆå§‹åŒ–FAISSå‘é‡åº“ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    @st.cache_resource
    def load_vector_store():
        try:
            embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
            vector_store = FAISS.load_local(
                folder_path=FAISS_INDEX_PATH,
                embeddings=embeddings,
                allow_dangerous_deserialization=True
            )
            return vector_store
        except Exception as e:
            st.error(f"åŠ è½½æ”¿ç­–çŸ¥è¯†åº“å¤±è´¥ï¼š{e}")
            st.stop()

    # 2. åˆå§‹åŒ–Agentï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    @st.cache_resource
    def load_agent():
        vector_store = load_vector_store()
        return build_agricultural_policy_agent(vector_store)

    # 3. åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆä¿å­˜å¯¹è¯å†å²å’ŒAgentçŠ¶æ€ï¼‰
    if "agent_state" not in st.session_state:
        st.session_state.agent_state = AgentState(messages=[])
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # åŠ è½½èµ„æº
    vector_store = load_vector_store()
    policy_agent = load_agent()

    # æ˜¾ç¤ºå†å²å¯¹è¯
    for msg in st.session_state.chat_history:
        if msg["role"] == "user":
            with st.chat_message("user"):
                st.markdown(msg["content"])
        else:
            with st.chat_message("assistant"):
                st.markdown(msg["content"])

    # ç”¨æˆ·è¾“å…¥æ¡†
    user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå¦‚ï¼šåŒ—äº¬å¸‚2025å¹´å°éº¦è¡¥è´´å¤šå°‘ï¼Ÿï¼‰")
    if user_input:
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        # è°ƒç”¨Agentå¤„ç†
        try:
            # æ›´æ–°AgentçŠ¶æ€
            st.session_state.agent_state.messages.append(HumanMessage(content=user_input))
            # æ‰§è¡ŒAgentå·¥ä½œæµ
            result = policy_agent.invoke(st.session_state.agent_state)
            # è½¬æ¢å›AgentStateå¯¹è±¡
            if isinstance(result, dict):
                st.session_state.agent_state = AgentState(**result)
            else:
                st.session_state.agent_state = result

            # è·å–å›ç­”å¹¶æ˜¾ç¤º
            answer = st.session_state.agent_state.final_answer
            with st.chat_message("assistant"):
                st.markdown(answer)
            st.session_state.chat_history.append({"role": "assistant", "content": answer})

        except Exception as e:
            st.error(f"å›ç­”ç”Ÿæˆå‡ºé”™ï¼š{str(e)}")
            # å›æ»šçŠ¶æ€
            st.session_state.agent_state.messages.pop()

    # ä¾§è¾¹æ ï¼šé‡ç½®å¯¹è¯
    with st.sidebar:
        st.header("âš™ï¸ åŠŸèƒ½è®¾ç½®")
        if st.button("æ¸…ç©ºå¯¹è¯å†å²", type="secondary"):
            st.session_state.agent_state = AgentState(messages=[])
            st.session_state.chat_history = []
            st.rerun()
        st.markdown("---")
        st.markdown("### ä½¿ç”¨è¯´æ˜ï¼š")
        st.markdown("1. æ”¯æŒæŸ¥è¯¢å„åœ°åŒºå†œä¸šè¡¥è´´æ”¿ç­–")
        st.markdown("2. æ”¯æŒè¯¢é—®è¡¥è´´ç”³è¯·æµç¨‹ã€èµ„æ ¼æ¡ä»¶")
        st.markdown("3. æ”¯æŒè®¡ç®—è¡¥è´´é‡‘é¢")
        st.markdown("4. æ”¯æŒå¤šè½®è¿½é—®ï¼Œé€æ­¥è¡¥å……ä¿¡æ¯")

# =========================
# å…¼å®¹CLIæ¨¡å¼ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
# =========================
def interactive_chat(agent, vector_store):
    """åŸæœ‰çš„CLIäº¤äº’æ¨¡å¼ï¼Œå¤‡ç”¨"""
    print("="*60)
    print("      å†œä¸šæ”¿ç­–æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰")
    print("="*60)

    current_state = AgentState(messages=[])

    while True:
        user_input = input("\nğŸ‘‰ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š").strip()

        if user_input.lower() in ["exit", "quit", "é€€å‡º", "ç»“æŸ"]:
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break

        if not user_input:
            print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼")
            continue

        current_state.messages.append(HumanMessage(content=user_input))

        try:
            result = agent.invoke(current_state)
            if isinstance(result, dict):
                current_state = AgentState(**result)
            else:
                current_state = result

            print("\nğŸ¤– å›ç­”ï¼š")
            print(current_state.final_answer)

        except Exception as e:
            print(f"\nâŒ å›ç­”ç”Ÿæˆå‡ºé”™ï¼š{e}")
            import traceback
            traceback.print_exc()
            current_state.messages.pop()

# =========================
# ä¸»ç¨‹åºå…¥å£
# =========================
if __name__ == "__main__":
    # é»˜è®¤å¯åŠ¨Streamlit Webç•Œé¢
    try:
        streamlit_chat_interface()
    # å¦‚æœç¯å¢ƒä¸æ”¯æŒStreamlitï¼ˆå¦‚æ— Webç¯å¢ƒï¼‰ï¼Œè‡ªåŠ¨é™çº§åˆ°CLIæ¨¡å¼
    except Exception as e:
        print(f"å¯åŠ¨Webç•Œé¢å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CLIæ¨¡å¼ï¼š{e}")
        # åŠ è½½å‘é‡åº“
        print("æ­£åœ¨åŠ è½½FAISSå‘é‡åº“...")
        try:
            embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
            vector_store = FAISS.load_local(
                folder_path=FAISS_INDEX_PATH,
                embeddings=embeddings,
                allow_dangerous_deserialization=True
            )
            print("âœ… FAISSå‘é‡åº“åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½FAISSå‘é‡åº“å¤±è´¥ï¼š{e}")

        # æ„å»ºAgentå¹¶å¯åŠ¨CLI
        policy_agent = build_agricultural_policy_agent(vector_store)
        interactive_chat(policy_agent, vector_store)